{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.optimizer import AdamNormGrad\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from utils.load_data import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='VAE+VampPrior')\n",
    "# arguments for optimization\n",
    "parser.add_argument('--batch_size', type=int, default=200, metavar='BStrain',\n",
    "                    help='input batch size for training (default: 200)')\n",
    "parser.add_argument('--test_batch_size', type=int, default=1000, metavar='BStest',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=400, metavar='E',\n",
    "                    help='number of epochs to train (default: 400)')\n",
    "parser.add_argument('--lr', type=float, default=0.0005, metavar='LR',\n",
    "                    help='learning rate (default: 0.0005)')\n",
    "parser.add_argument('--early_stopping_epochs', type=int, default=50, metavar='ES',\n",
    "                    help='number of epochs for early stopping')\n",
    "\n",
    "parser.add_argument('--warmup', type=int, default=100, metavar='WU',\n",
    "                    help='number of epochs for warm-up')\n",
    "parser.add_argument('--max_beta', type=float, default=1., metavar='B',\n",
    "                    help='maximum value of beta for training')\n",
    "\n",
    "# cuda\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "# random seed\n",
    "parser.add_argument('--seed', type=int, default=14, metavar='S',\n",
    "                    help='random seed (default: 14)')\n",
    "\n",
    "# model: latent size, input_size, so on\n",
    "parser.add_argument('--num_layers', type=int, default=1, metavar='NL',\n",
    "                    help='number of layers')\n",
    "\n",
    "parser.add_argument('--z1_size', type=int, default=200, metavar='M1',\n",
    "                    help='latent size')\n",
    "parser.add_argument('--z2_size', type=int, default=200, metavar='M2',\n",
    "                    help='latent size')\n",
    "parser.add_argument('--hidden_size', type=int, default=600 , metavar=\"H\",\n",
    "                    help='the width of hidden layers')\n",
    "parser.add_argument('--input_size', type=int, default=[1, 28, 28], metavar='D',\n",
    "                    help='input size')\n",
    "\n",
    "parser.add_argument('--activation', type=str, default=None, metavar='ACT',\n",
    "                    help='activation function')\n",
    "\n",
    "parser.add_argument('--number_components', type=int, default=1000, metavar='NC',\n",
    "                    help='number of pseudo-inputs')\n",
    "parser.add_argument('--pseudoinputs_mean', type=float, default=0.05, metavar='PM',\n",
    "                    help='mean for init pseudo-inputs')\n",
    "parser.add_argument('--pseudoinputs_std', type=float, default=0.01, metavar='PS',\n",
    "                    help='std for init pseudo-inputs')\n",
    "\n",
    "parser.add_argument('--use_training_data_init', action='store_true', default=False,\n",
    "                    help='initialize pseudo-inputs with randomly chosen training data')\n",
    "\n",
    "# model: model name, prior\n",
    "parser.add_argument('--model_name', type=str, default='vamp', metavar='MN',\n",
    "                    help='model name: baseline, vamp, hvamp, hvamp1')\n",
    "\n",
    "parser.add_argument('--input_type', type=str, default='binary', metavar='IT',\n",
    "                    help='type of the input: binary, gray, continuous, multinomial')\n",
    "\n",
    "parser.add_argument('--gated', action='store_true', default=False,\n",
    "                    help='use gating mechanism')\n",
    "\n",
    "# experiment\n",
    "parser.add_argument('--S', type=int, default=5000, metavar='SLL',\n",
    "                    help='number of samples used for approximating log-likelihood')\n",
    "parser.add_argument('--MB', type=int, default=100, metavar='MBLL',\n",
    "                    help='size of a mini-batch used for approximating log-likelihood')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--dataset_name', type=str, default='netflix', metavar='DN',\n",
    "                    help='name of the dataset:  ml20m, netflix, pinterest')\n",
    "\n",
    "parser.add_argument('--dynamic_binarization', action='store_true', default=False,\n",
    "                    help='allow dynamic binarization')\n",
    "\n",
    "# note\n",
    "parser.add_argument('--note', type=str, default=\"none\", metavar='NT',\n",
    "                    help='additional note on the experiment')\n",
    "parser.add_argument('--no_log', action='store_true', default=False,\n",
    "                    help='print log to log_dir')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}  #! Changed num_workers: 1->0 because of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28, 28]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('netflix', 'vamp')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_name,args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "args.model_signature = str(datetime.datetime.now())[0:10]\n",
    "\n",
    "model_name = args.dataset_name + '_' + args.model_name + '_' + \\\n",
    "                '(K_' + str(args.number_components) + ')' + \\\n",
    "                '_' + args.input_type + '_beta(' + str(args.max_beta) + ')' + \\\n",
    "                '_layers(' + str(args.num_layers) + ')' + '_hidden(' + str(args.hidden_size) + ')' + \\\n",
    "                '_z1(' + str(args.z1_size) + ')' + '_z2(' + str(args.z2_size) + ')'\n",
    "\n",
    "# DIRECTORY FOR SAVING\n",
    "snapshots_path = 'snapshots/'\n",
    "dir = snapshots_path + args.model_signature + '_' + model_name + '/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# LOAD DATA=========================================================================================================\n",
    "print('load data')\n",
    "\n",
    "# loading data\n",
    "train_loader, val_loader, test_loader, args = load_dataset(args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model\n",
      "Namespace(MB=100, S=5000, activation=None, batch_size=200, cuda=False, dataset_name='netflix', dynamic_binarization=False, early_stopping_epochs=50, epochs=400, gated=False, hidden_size=600, input_size=[1, 1, 7738], input_type='binary', lr=0.0005, max_beta=1.0, model_name='vamp', model_signature='2023-05-14', no_cuda=False, no_log=False, note='none', num_layers=1, number_components=1000, pseudoinputs_mean=0.05, pseudoinputs_std=0.01, seed=14, test_batch_size=1000, use_training_data_init=False, warmup=100, z1_size=200, z2_size=200)\n"
     ]
    }
   ],
   "source": [
    "#def run(args, kwargs):\n",
    "\n",
    "# CREATE MODEL======================================================================================================\n",
    "print('create model')\n",
    "# importing model\n",
    "if args.model_name == 'baseline':\n",
    "    from models.Baseline import VAE\n",
    "elif args.model_name == 'vamp':\n",
    "    from models.Vamp import VAE\n",
    "elif args.model_name == 'hvamp':\n",
    "    from models.HVamp import VAE\n",
    "elif args.model_name == 'hvamp1':\n",
    "    from models.HVamp_1layer import VAE\n",
    "else:\n",
    "    raise Exception('Wrong name of the model!')\n",
    "\n",
    "model = VAE(args)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = AdamNormGrad(model.parameters(), lr=args.lr)\n",
    "\n",
    "# ======================================================================================================================\n",
    "print(args)\n",
    "log_dir = \"vae_experiment_log_\" + str(os.getenv(\"COMPUTERNAME\")) +\".txt\"\n",
    "\n",
    "open(log_dir, 'a').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import train_vae as train\n",
    "from utils.evaluation import evaluate_vae as evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================================================================\n",
    "print('perform experiment')\n",
    "model_name = args.model_name\n",
    "dataset_name = args.dataset_name\n",
    "\n",
    "torch.save(args, dir + args.model_name + '.config')\n",
    "\n",
    "# best_model = model\n",
    "best_ndcg = 0.\n",
    "e = 0\n",
    "last_epoch = 0\n",
    "\n",
    "train_loss_history = []\n",
    "train_re_history = []\n",
    "train_kl_history = []\n",
    "\n",
    "val_loss_history = []\n",
    "val_re_history = []\n",
    "val_kl_history = []\n",
    "\n",
    "val_ndcg_history = []\n",
    "\n",
    "time_history = []\n",
    "# ======================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 7738]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (q_z_layers): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=7738, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_z_mean): Linear(in_features=600, out_features=200, bias=True)\n",
       "  (q_z_logvar): NonLinear(\n",
       "    (activation): Hardtanh(min_val=-12.0, max_val=4.0)\n",
       "    (linear): Linear(in_features=600, out_features=200, bias=True)\n",
       "  )\n",
       "  (p_x_layers): Sequential(\n",
       "    (0): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=200, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (p_x_mean): NonLinear(\n",
       "    (activation): Sigmoid()\n",
       "    (linear): Linear(in_features=600, out_features=7738, bias=True)\n",
       "  )\n",
       "  (means): NonLinear(\n",
       "    (activation): Hardtanh(min_val=0.0, max_val=1.0)\n",
       "    (linear): Linear(in_features=1000, out_features=7738, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import bottleneck as bn\n",
    "def evaluate_vae(args, model, train_loader, data_loader, epoch, dir, mode):\n",
    "    # set loss to 0\n",
    "    evaluate_loss = 0\n",
    "    evaluate_re = 0\n",
    "    evaluate_kl = 0\n",
    "\n",
    "    ndcg_dist = torch.tensor([], dtype=torch.float)\n",
    "    if mode == 'test':\n",
    "        ndcg_20 = torch.tensor([], dtype=torch.float)\n",
    "        ndcg_10 = torch.tensor([], dtype=torch.float)\n",
    "        recall_50 = torch.tensor([], dtype=torch.float)\n",
    "        recall_20 = torch.tensor([], dtype=torch.float)\n",
    "        recall_10 = torch.tensor([], dtype=torch.float)\n",
    "        recall_5 = torch.tensor([], dtype=torch.float)\n",
    "        recall_1 = torch.tensor([], dtype=torch.float)\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Functions for Evaluation\n",
    "\n",
    "    def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "        '''\n",
    "        normalized discounted cumulative gain@k for binary relevance\n",
    "        ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "        '''\n",
    "        batch_users = X_pred.shape[0]\n",
    "        idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "        topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                           idx_topk_part[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "        # topk predicted score\n",
    "        idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "        # build the discount template\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        tp = torch.tensor(tp, dtype=torch.float)  # ! in order to do operations with torch tensor\n",
    "\n",
    "        DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                             idx_topk].cpu() * tp).sum(dim=1)\n",
    "        IDCG = torch.tensor([(tp[:min(n, k)]).sum()\n",
    "                             for n in (heldout_batch != 0).sum(dim=1)])\n",
    "        return DCG / IDCG\n",
    "\n",
    "    def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "        batch_users = X_pred.shape[0]\n",
    "\n",
    "        idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "        X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "        X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "        X_true_binary = torch.tensor((heldout_batch > 0), dtype=torch.float)\n",
    "        tmp = torch.tensor(np.logical_and(X_true_binary, X_pred_binary), dtype=torch.float).sum(dim=1)\n",
    "        recall = tmp / np.minimum(k, X_true_binary.sum(dim=1))\n",
    "        return recall\n",
    "\n",
    "    # evaluate\n",
    "    for batch_idx, (train, test) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            train, test = train.cuda(), test.cuda()\n",
    "        train, test = Variable(train), Variable(test) #! volatile deprecated\n",
    "\n",
    "        x = train\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # calculate loss function\n",
    "            loss, RE, KL = model.calculate_loss(x, average=True)\n",
    "\n",
    "            evaluate_loss += loss.data.item()\n",
    "            evaluate_re += -RE.data.item()\n",
    "            evaluate_kl += KL.data.item()\n",
    "\n",
    "            # Calculate NDCG & Recall\n",
    "            pred_val = model.reconstruct_x(x)\n",
    "            # should be removed if not necessary\n",
    "            pred_val = np.array(pred_val)\n",
    "            x = np.array(x)\n",
    "            pred_val[x.nonzero()] = -np.inf\n",
    "\n",
    "            ndcg_dist = torch.cat([ndcg_dist, NDCG_binary_at_k_batch(pred_val, test, k=100)])\n",
    "\n",
    "            if mode == 'test':\n",
    "                ndcg_20 = torch.cat([ndcg_20, NDCG_binary_at_k_batch(pred_val, test, k=20)])\n",
    "                ndcg_10 = torch.cat([ndcg_10, NDCG_binary_at_k_batch(pred_val, test, k=10)])\n",
    "                recall_50 = torch.cat([recall_50, Recall_at_k_batch(pred_val, test, k=50)])\n",
    "                recall_20 = torch.cat([recall_20, Recall_at_k_batch(pred_val, test, k=20)])\n",
    "                recall_10 = torch.cat([recall_10, Recall_at_k_batch(pred_val, test, k=10)])\n",
    "                recall_5 = torch.cat([recall_5, Recall_at_k_batch(pred_val, test, k=5)])\n",
    "                recall_1 = torch.cat([recall_1, Recall_at_k_batch(pred_val, test, k=1)])\n",
    "\n",
    "\n",
    "\n",
    "    # calculate final loss\n",
    "    evaluate_loss /= len(data_loader)  # loss function already averages over batch size\n",
    "    evaluate_re /= len(data_loader)  # re already averages over batch size\n",
    "    evaluate_kl /= len(data_loader)  # kl already averages over batch size\n",
    "    print(ndcg_dist)\n",
    "    evaluate_ndcg = ndcg_dist.nanmean().data.item()\n",
    "\n",
    "    if mode == 'test':\n",
    "        eval_ndcg100 = \"{:.5f}({:.4f})\".format(evaluate_ndcg, ndcg_dist.std().data.item()/np.sqrt(len(ndcg_dist)))\n",
    "        eval_ndcg20 = \"{:.5f}({:.4f})\".format(ndcg_20.mean().data.item(),ndcg_20.std().data.item()/np.sqrt(len(ndcg_20)))\n",
    "        eval_ndcg10 = \"{:.5f}({:.4f})\".format(ndcg_10.mean().data.item(),ndcg_10.std().data.item()/np.sqrt(len(ndcg_10)))\n",
    "        eval_recall50 = \"{:.5f}({:.4f})\".format(recall_50.mean().data.item(),recall_50.std().data.item()/np.sqrt(len(recall_50)))\n",
    "        eval_recall20 = \"{:.5f}({:.4f})\".format(recall_20.mean().data.item(),recall_20.std().data.item()/np.sqrt(len(recall_20)))\n",
    "        eval_recall10 = \"{:.5f}({:.4f})\".format(recall_10.mean().data.item(),recall_10.std().data.item()/np.sqrt(len(recall_10)))\n",
    "        eval_recall5 = \"{:.5f}({:.4f})\".format(recall_5.mean().data.item(),recall_5.std().data.item()/np.sqrt(len(recall_5)))\n",
    "        eval_recall1 = \"{:.5f}({:.4f})\".format(recall_1.mean().data.item(),recall_1.std().data.item()/np.sqrt(len(recall_1)))\n",
    "\n",
    "\n",
    "\n",
    "    if mode == 'test':\n",
    "        return evaluate_loss, evaluate_re, evaluate_kl, eval_ndcg100, \\\n",
    "               eval_ndcg20, eval_ndcg10, eval_recall50, eval_recall20, eval_recall10, eval_recall5, eval_recall1\n",
    "    else:\n",
    "        return evaluate_loss, evaluate_re, evaluate_kl, evaluate_ndcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.01\n",
      "tensor([0.0878,    nan,    nan,  ...,    nan,    nan, 0.2906])\n",
      "Epoch: 1/2, Time elapsed: 114.38s\n",
      "* Train loss: 113.33   (RE: 113.25, KL: 7.60)\n",
      "o Val.  loss: 93.45   (RE: 90.32, KL: 3.13, NDCG: 0.15126)\n",
      "--> Early stopping: 0/50 (BEST: 0.00000)\n",
      "\n",
      "->model saved<-\n",
      "beta: 0.02\n",
      "tensor([0.0971,    nan,    nan,  ...,    nan,    nan, 0.3109])\n",
      "Epoch: 2/2, Time elapsed: 111.72s\n",
      "* Train loss: 97.44   (RE: 97.20, KL: 11.80)\n",
      "o Val.  loss: 83.37   (RE: 76.29, KL: 7.08, NDCG: 0.15711)\n",
      "--> Early stopping: 0/50 (BEST: 0.15126)\n",
      "\n",
      "->model saved<-\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "args.epochs = 2\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    time_start = time.time()\n",
    "    model, train_loss_epoch, train_re_epoch, train_kl_epoch = train(epoch, args, train_loader, model,\n",
    "                                                                            optimizer)\n",
    "\n",
    "    val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch = evaluate_vae(args, model, train_loader, val_loader, epoch, dir, mode='validation')\n",
    "    time_end = time.time()\n",
    "\n",
    "    time_elapsed = time_end - time_start\n",
    "\n",
    "    # appending history\n",
    "    train_loss_history.append(train_loss_epoch), train_re_history.append(train_re_epoch), train_kl_history.append(\n",
    "        train_kl_epoch)\n",
    "    val_loss_history.append(val_loss_epoch), val_re_history.append(val_re_epoch), val_kl_history.append(\n",
    "        val_kl_epoch), val_ndcg_history.append(val_ndcg_epoch)\n",
    "    time_history.append(time_elapsed)\n",
    "\n",
    "    # printing results\n",
    "    print('Epoch: {}/{}, Time elapsed: {:.2f}s\\n'\n",
    "            '* Train loss: {:.2f}   (RE: {:.2f}, KL: {:.2f})\\n'\n",
    "            'o Val.  loss: {:.2f}   (RE: {:.2f}, KL: {:.2f}, NDCG: {:.5f})\\n'\n",
    "            '--> Early stopping: {}/{} (BEST: {:.5f})\\n'.format(\n",
    "        epoch, args.epochs, time_elapsed,\n",
    "        train_loss_epoch, train_re_epoch, train_kl_epoch,\n",
    "        val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch,\n",
    "        e, args.early_stopping_epochs, best_ndcg\n",
    "    ))\n",
    "\n",
    "    # early-stopping\n",
    "    last_epoch = epoch\n",
    "    if val_ndcg_epoch > best_ndcg:\n",
    "        e = 0\n",
    "        best_ndcg = val_ndcg_epoch\n",
    "        # best_model = model\n",
    "        print('->model saved<-')\n",
    "        torch.save(model, dir + args.model_name + '.model')\n",
    "    else:\n",
    "        e += 1\n",
    "        if epoch < args.warmup:\n",
    "            e = 0\n",
    "        if e > args.early_stopping_epochs:\n",
    "            break\n",
    "\n",
    "    # NaN\n",
    "    if math.isnan(val_loss_epoch):\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUBI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_name= \"mubi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mubi', 'vamp')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_name,args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.4 GiB for an array with shape (40000, 69554) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\main.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mload data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# loading data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_loader, val_loader, test_loader, args \u001b[39m=\u001b[39m load_dataset(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\utils\\load_data.py:474\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(args, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m     train_loader, val_loader, test_loader, args \u001b[39m=\u001b[39m load_netflix(args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    473\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mdataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmubi\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 474\u001b[0m     train_loader, val_loader, test_loader, args \u001b[39m=\u001b[39m load_mubi(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    475\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mdataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpinterest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    476\u001b[0m     train_loader, val_loader, test_loader, args \u001b[39m=\u001b[39m load_pinterest(args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\utils\\load_data.py:213\u001b[0m, in \u001b[0;36mload_mubi\u001b[1;34m(args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m x_train \u001b[39m=\u001b[39m load_train_data(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmubi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    212\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(x_train)\n\u001b[1;32m--> 213\u001b[0m x_val_tr, x_val_te \u001b[39m=\u001b[39m load_tr_te_data(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m\"\u001b[39;49m\u001b[39mdatasets\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmubi\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalidation_tr.csv\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    214\u001b[0m                                      os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m\"\u001b[39;49m\u001b[39mdatasets\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmubi\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalidation_te.csv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    216\u001b[0m x_test_tr, x_test_te \u001b[39m=\u001b[39m load_tr_te_data(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmubi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_tr.csv\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    217\u001b[0m                                        os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmubi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_te.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    219\u001b[0m \u001b[39m# idle y's\u001b[39;00m\n",
      "File \u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\utils\\load_data.py:205\u001b[0m, in \u001b[0;36mload_mubi.<locals>.load_tr_te_data\u001b[1;34m(csv_file_tr, csv_file_te)\u001b[0m\n\u001b[0;32m    201\u001b[0m rows_te, cols_te \u001b[39m=\u001b[39m tp_te[\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m start_idx, tp_te[\u001b[39m'\u001b[39m\u001b[39msid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    203\u001b[0m data_tr \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39mcsr_matrix((np\u001b[39m.\u001b[39mones_like(rows_tr),(rows_tr, cols_tr)),\n\u001b[0;32m    204\u001b[0m                             dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m, shape\u001b[39m=\u001b[39m(end_idx \u001b[39m-\u001b[39m start_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, n_items))\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m--> 205\u001b[0m data_te \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39;49mcsr_matrix((np\u001b[39m.\u001b[39;49mones_like(rows_te),(rows_te, cols_te)),\n\u001b[0;32m    206\u001b[0m                             dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m, shape\u001b[39m=\u001b[39;49m(end_idx \u001b[39m-\u001b[39;49m start_idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, n_items))\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m data_tr, data_te\n",
      "File \u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\vae\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\vae\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.4 GiB for an array with shape (40000, 69554) and data type float32"
     ]
    }
   ],
   "source": [
    "args.model_signature = str(datetime.datetime.now())[0:10]\n",
    "\n",
    "model_name = args.dataset_name + '_' + args.model_name + '_' + \\\n",
    "                '(K_' + str(args.number_components) + ')' + \\\n",
    "                '_' + args.input_type + '_beta(' + str(args.max_beta) + ')' + \\\n",
    "                '_layers(' + str(args.num_layers) + ')' + '_hidden(' + str(args.hidden_size) + ')' + \\\n",
    "                '_z1(' + str(args.z1_size) + ')' + '_z2(' + str(args.z2_size) + ')'\n",
    "\n",
    "# DIRECTORY FOR SAVING\n",
    "snapshots_path = 'snapshots/'\n",
    "dir = snapshots_path + args.model_signature + '_' + model_name + '/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# LOAD DATA=========================================================================================================\n",
    "print('load data')\n",
    "\n",
    "# loading data\n",
    "train_loader, val_loader, test_loader, args = load_dataset(args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model\n",
      "Namespace(MB=100, S=5000, activation=None, batch_size=200, cuda=False, dataset_name='netflix', dynamic_binarization=False, early_stopping_epochs=50, epochs=400, gated=False, hidden_size=600, input_size=[1, 1, 7738], input_type='binary', lr=0.0005, max_beta=1.0, model_name='vamp', model_signature='2023-05-14', no_cuda=False, no_log=False, note='none', num_layers=1, number_components=1000, pseudoinputs_mean=0.05, pseudoinputs_std=0.01, seed=14, test_batch_size=1000, use_training_data_init=False, warmup=100, z1_size=200, z2_size=200)\n"
     ]
    }
   ],
   "source": [
    "#def run(args, kwargs):\n",
    "\n",
    "# CREATE MODEL======================================================================================================\n",
    "print('create model')\n",
    "# importing model\n",
    "if args.model_name == 'baseline':\n",
    "    from models.Baseline import VAE\n",
    "elif args.model_name == 'vamp':\n",
    "    from models.Vamp import VAE\n",
    "elif args.model_name == 'hvamp':\n",
    "    from models.HVamp import VAE\n",
    "elif args.model_name == 'hvamp1':\n",
    "    from models.HVamp_1layer import VAE\n",
    "else:\n",
    "    raise Exception('Wrong name of the model!')\n",
    "\n",
    "model = VAE(args)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = AdamNormGrad(model.parameters(), lr=args.lr)\n",
    "\n",
    "# ======================================================================================================================\n",
    "print(args)\n",
    "log_dir = \"vae_experiment_log_\" + str(os.getenv(\"COMPUTERNAME\")) +\".txt\"\n",
    "\n",
    "open(log_dir, 'a').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import train_vae as train\n",
    "from utils.evaluation import evaluate_vae as evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================================================================\n",
    "print('perform experiment')\n",
    "model_name = args.model_name\n",
    "dataset_name = args.dataset_name\n",
    "\n",
    "torch.save(args, dir + args.model_name + '.config')\n",
    "\n",
    "# best_model = model\n",
    "best_ndcg = 0.\n",
    "e = 0\n",
    "last_epoch = 0\n",
    "\n",
    "train_loss_history = []\n",
    "train_re_history = []\n",
    "train_kl_history = []\n",
    "\n",
    "val_loss_history = []\n",
    "val_re_history = []\n",
    "val_kl_history = []\n",
    "\n",
    "val_ndcg_history = []\n",
    "\n",
    "time_history = []\n",
    "# ======================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 7738]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (q_z_layers): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=7738, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_z_mean): Linear(in_features=600, out_features=200, bias=True)\n",
       "  (q_z_logvar): NonLinear(\n",
       "    (activation): Hardtanh(min_val=-12.0, max_val=4.0)\n",
       "    (linear): Linear(in_features=600, out_features=200, bias=True)\n",
       "  )\n",
       "  (p_x_layers): Sequential(\n",
       "    (0): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=200, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (p_x_mean): NonLinear(\n",
       "    (activation): Sigmoid()\n",
       "    (linear): Linear(in_features=600, out_features=7738, bias=True)\n",
       "  )\n",
       "  (means): NonLinear(\n",
       "    (activation): Hardtanh(min_val=0.0, max_val=1.0)\n",
       "    (linear): Linear(in_features=1000, out_features=7738, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\github\\VAE-based-Collaborative-Filtering\\utils\\optimizer.py:75: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1485.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32mf:\\github\\VAE-based-Collaborative-Filtering\\main.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model, train_loss_epoch, train_re_epoch, train_kl_epoch \u001b[39m=\u001b[39m train(epoch, args, train_loader, model,\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                                         optimizer)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch \u001b[39m=\u001b[39m evaluate_vae(args, model, train_loader, val_loader, epoch, \u001b[39mdir\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/github/VAE-based-Collaborative-Filtering/main.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m time_elapsed \u001b[39m=\u001b[39m time_end \u001b[39m-\u001b[39m time_start\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_vae' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "args.epochs = 2\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    time_start = time.time()\n",
    "    model, train_loss_epoch, train_re_epoch, train_kl_epoch = train(epoch, args, train_loader, model,\n",
    "                                                                            optimizer)\n",
    "\n",
    "    val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch = evaluate_vae(args, model, train_loader, val_loader, epoch, dir, mode='validation')\n",
    "    time_end = time.time()\n",
    "\n",
    "    time_elapsed = time_end - time_start\n",
    "\n",
    "    # appending history\n",
    "    train_loss_history.append(train_loss_epoch), train_re_history.append(train_re_epoch), train_kl_history.append(\n",
    "        train_kl_epoch)\n",
    "    val_loss_history.append(val_loss_epoch), val_re_history.append(val_re_epoch), val_kl_history.append(\n",
    "        val_kl_epoch), val_ndcg_history.append(val_ndcg_epoch)\n",
    "    time_history.append(time_elapsed)\n",
    "\n",
    "    # printing results\n",
    "    print('Epoch: {}/{}, Time elapsed: {:.2f}s\\n'\n",
    "            '* Train loss: {:.2f}   (RE: {:.2f}, KL: {:.2f})\\n'\n",
    "            'o Val.  loss: {:.2f}   (RE: {:.2f}, KL: {:.2f}, NDCG: {:.5f})\\n'\n",
    "            '--> Early stopping: {}/{} (BEST: {:.5f})\\n'.format(\n",
    "        epoch, args.epochs, time_elapsed,\n",
    "        train_loss_epoch, train_re_epoch, train_kl_epoch,\n",
    "        val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch,\n",
    "        e, args.early_stopping_epochs, best_ndcg\n",
    "    ))\n",
    "\n",
    "    # early-stopping\n",
    "    last_epoch = epoch\n",
    "    if val_ndcg_epoch > best_ndcg:\n",
    "        e = 0\n",
    "        best_ndcg = val_ndcg_epoch\n",
    "        # best_model = model\n",
    "        print('->model saved<-')\n",
    "        torch.save(model, dir + args.model_name + '.model')\n",
    "    else:\n",
    "        e += 1\n",
    "        if epoch < args.warmup:\n",
    "            e = 0\n",
    "        if e > args.early_stopping_epochs:\n",
    "            break\n",
    "\n",
    "    # NaN\n",
    "    if math.isnan(val_loss_epoch):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch = evaluate_vae(args, model, train_loader, val_loader, epoch, dir, mode='validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02525036782026291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_ndcg_epoch\n",
    "#evaluate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
