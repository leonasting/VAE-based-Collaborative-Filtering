{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.optimizer import AdamNormGrad\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from utils.load_data import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='VAE+VampPrior')\n",
    "# arguments for optimization\n",
    "parser.add_argument('--batch_size', type=int, default=200, metavar='BStrain',\n",
    "                    help='input batch size for training (default: 200)')\n",
    "parser.add_argument('--test_batch_size', type=int, default=1000, metavar='BStest',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=400, metavar='E',\n",
    "                    help='number of epochs to train (default: 400)')\n",
    "parser.add_argument('--lr', type=float, default=0.0005, metavar='LR',\n",
    "                    help='learning rate (default: 0.0005)')\n",
    "parser.add_argument('--early_stopping_epochs', type=int, default=50, metavar='ES',\n",
    "                    help='number of epochs for early stopping')\n",
    "\n",
    "parser.add_argument('--warmup', type=int, default=100, metavar='WU',\n",
    "                    help='number of epochs for warm-up')\n",
    "parser.add_argument('--max_beta', type=float, default=1., metavar='B',\n",
    "                    help='maximum value of beta for training')\n",
    "\n",
    "# cuda\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "# random seed\n",
    "parser.add_argument('--seed', type=int, default=14, metavar='S',\n",
    "                    help='random seed (default: 14)')\n",
    "\n",
    "# model: latent size, input_size, so on\n",
    "parser.add_argument('--num_layers', type=int, default=1, metavar='NL',\n",
    "                    help='number of layers')\n",
    "\n",
    "parser.add_argument('--z1_size', type=int, default=200, metavar='M1',\n",
    "                    help='latent size')\n",
    "parser.add_argument('--z2_size', type=int, default=200, metavar='M2',\n",
    "                    help='latent size')\n",
    "parser.add_argument('--hidden_size', type=int, default=600 , metavar=\"H\",\n",
    "                    help='the width of hidden layers')\n",
    "parser.add_argument('--input_size', type=int, default=[1, 28, 28], metavar='D',\n",
    "                    help='input size')\n",
    "\n",
    "parser.add_argument('--activation', type=str, default=None, metavar='ACT',\n",
    "                    help='activation function')\n",
    "\n",
    "parser.add_argument('--number_components', type=int, default=1000, metavar='NC',\n",
    "                    help='number of pseudo-inputs')\n",
    "parser.add_argument('--pseudoinputs_mean', type=float, default=0.05, metavar='PM',\n",
    "                    help='mean for init pseudo-inputs')\n",
    "parser.add_argument('--pseudoinputs_std', type=float, default=0.01, metavar='PS',\n",
    "                    help='std for init pseudo-inputs')\n",
    "\n",
    "parser.add_argument('--use_training_data_init', action='store_true', default=False,\n",
    "                    help='initialize pseudo-inputs with randomly chosen training data')\n",
    "\n",
    "# model: model name, prior\n",
    "parser.add_argument('--model_name', type=str, default='vamp', metavar='MN',\n",
    "                    help='model name: baseline, vamp, hvamp, hvamp1')\n",
    "\n",
    "parser.add_argument('--input_type', type=str, default='binary', metavar='IT',\n",
    "                    help='type of the input: binary, gray, continuous, multinomial')\n",
    "\n",
    "parser.add_argument('--gated', action='store_true', default=False,\n",
    "                    help='use gating mechanism')\n",
    "\n",
    "# experiment\n",
    "parser.add_argument('--S', type=int, default=5000, metavar='SLL',\n",
    "                    help='number of samples used for approximating log-likelihood')\n",
    "parser.add_argument('--MB', type=int, default=100, metavar='MBLL',\n",
    "                    help='size of a mini-batch used for approximating log-likelihood')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--dataset_name', type=str, default='netflix', metavar='DN',\n",
    "                    help='name of the dataset:  ml20m, netflix, pinterest')\n",
    "\n",
    "parser.add_argument('--dynamic_binarization', action='store_true', default=False,\n",
    "                    help='allow dynamic binarization')\n",
    "\n",
    "# note\n",
    "parser.add_argument('--note', type=str, default=\"none\", metavar='NT',\n",
    "                    help='additional note on the experiment')\n",
    "parser.add_argument('--no_log', action='store_true', default=False,\n",
    "                    help='print log to log_dir')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}  #! Changed num_workers: 1->0 because of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28, 28]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('netflix', 'vamp')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_name,args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "args.model_signature = str(datetime.datetime.now())[0:10]\n",
    "\n",
    "model_name = args.dataset_name + '_' + args.model_name + '_' + \\\n",
    "                '(K_' + str(args.number_components) + ')' + \\\n",
    "                '_' + args.input_type + '_beta(' + str(args.max_beta) + ')' + \\\n",
    "                '_layers(' + str(args.num_layers) + ')' + '_hidden(' + str(args.hidden_size) + ')' + \\\n",
    "                '_z1(' + str(args.z1_size) + ')' + '_z2(' + str(args.z2_size) + ')'\n",
    "\n",
    "# DIRECTORY FOR SAVING\n",
    "snapshots_path = 'snapshots/'\n",
    "dir = snapshots_path + args.model_signature + '_' + model_name + '/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# LOAD DATA=========================================================================================================\n",
    "print('load data')\n",
    "\n",
    "# loading data\n",
    "train_loader, val_loader, test_loader, args = load_dataset(args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model\n",
      "Namespace(MB=100, S=5000, activation=None, batch_size=200, cuda=False, dataset_name='netflix', dynamic_binarization=False, early_stopping_epochs=50, epochs=400, gated=False, hidden_size=600, input_size=[1, 1, 7738], input_type='binary', lr=0.0005, max_beta=1.0, model_name='vamp', model_signature='2023-05-12', no_cuda=False, no_log=False, note='none', num_layers=1, number_components=1000, pseudoinputs_mean=0.05, pseudoinputs_std=0.01, seed=14, test_batch_size=1000, use_training_data_init=False, warmup=100, z1_size=200, z2_size=200)\n"
     ]
    }
   ],
   "source": [
    "#def run(args, kwargs):\n",
    "\n",
    "# CREATE MODEL======================================================================================================\n",
    "print('create model')\n",
    "# importing model\n",
    "if args.model_name == 'baseline':\n",
    "    from models.Baseline import VAE\n",
    "elif args.model_name == 'vamp':\n",
    "    from models.Vamp import VAE\n",
    "elif args.model_name == 'hvamp':\n",
    "    from models.HVamp import VAE\n",
    "elif args.model_name == 'hvamp1':\n",
    "    from models.HVamp_1layer import VAE\n",
    "else:\n",
    "    raise Exception('Wrong name of the model!')\n",
    "\n",
    "model = VAE(args)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = AdamNormGrad(model.parameters(), lr=args.lr)\n",
    "\n",
    "# ======================================================================================================================\n",
    "print(args)\n",
    "log_dir = \"vae_experiment_log_\" + str(os.getenv(\"COMPUTERNAME\")) +\".txt\"\n",
    "\n",
    "open(log_dir, 'a').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import train_vae as train\n",
    "from utils.evaluation import evaluate_vae as evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================================================================\n",
    "print('perform experiment')\n",
    "model_name = args.model_name\n",
    "dataset_name = args.dataset_name\n",
    "\n",
    "torch.save(args, dir + args.model_name + '.config')\n",
    "\n",
    "# best_model = model\n",
    "best_ndcg = 0.\n",
    "e = 0\n",
    "last_epoch = 0\n",
    "\n",
    "train_loss_history = []\n",
    "train_re_history = []\n",
    "train_kl_history = []\n",
    "\n",
    "val_loss_history = []\n",
    "val_re_history = []\n",
    "val_kl_history = []\n",
    "\n",
    "val_ndcg_history = []\n",
    "\n",
    "time_history = []\n",
    "# ======================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 7738]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (q_z_layers): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=7738, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_z_mean): Linear(in_features=600, out_features=200, bias=True)\n",
       "  (q_z_logvar): NonLinear(\n",
       "    (activation): Hardtanh(min_val=-12.0, max_val=4.0)\n",
       "    (linear): Linear(in_features=600, out_features=200, bias=True)\n",
       "  )\n",
       "  (p_x_layers): Sequential(\n",
       "    (0): NonLinear(\n",
       "      (activation): Tanh()\n",
       "      (linear): Linear(in_features=200, out_features=600, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (p_x_mean): NonLinear(\n",
       "    (activation): Sigmoid()\n",
       "    (linear): Linear(in_features=600, out_features=7738, bias=True)\n",
       "  )\n",
       "  (means): NonLinear(\n",
       "    (activation): Hardtanh(min_val=0.0, max_val=1.0)\n",
       "    (linear): Linear(in_features=1000, out_features=7738, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.01\n",
      "Epoch: 1/2, Time elapsed: 124.47s\n",
      "* Train loss: 202.76   (RE: 202.70, KL: 5.50)\n",
      "o Val.  loss: 105.93   (RE: 106.55, KL: -0.61, NDCG: 0.13753)\n",
      "--> Early stopping: 0/50 (BEST: 0.00000)\n",
      "\n",
      "->model saved<-\n",
      "beta: 0.02\n",
      "Epoch: 2/2, Time elapsed: 106.37s\n",
      "* Train loss: 114.04   (RE: 113.88, KL: 7.85)\n",
      "o Val.  loss: 94.55   (RE: 91.85, KL: 2.70, NDCG: 0.14973)\n",
      "--> Early stopping: 0/50 (BEST: 0.13753)\n",
      "\n",
      "->model saved<-\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "args.epochs = 2\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    time_start = time.time()\n",
    "    model, train_loss_epoch, train_re_epoch, train_kl_epoch = train(epoch, args, train_loader, model,\n",
    "                                                                            optimizer)\n",
    "\n",
    "    val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch = evaluate_vae(args, model, train_loader, val_loader, epoch, dir, mode='validation')\n",
    "    time_end = time.time()\n",
    "\n",
    "    time_elapsed = time_end - time_start\n",
    "\n",
    "    # appending history\n",
    "    train_loss_history.append(train_loss_epoch), train_re_history.append(train_re_epoch), train_kl_history.append(\n",
    "        train_kl_epoch)\n",
    "    val_loss_history.append(val_loss_epoch), val_re_history.append(val_re_epoch), val_kl_history.append(\n",
    "        val_kl_epoch), val_ndcg_history.append(val_ndcg_epoch)\n",
    "    time_history.append(time_elapsed)\n",
    "\n",
    "    # printing results\n",
    "    print('Epoch: {}/{}, Time elapsed: {:.2f}s\\n'\n",
    "            '* Train loss: {:.2f}   (RE: {:.2f}, KL: {:.2f})\\n'\n",
    "            'o Val.  loss: {:.2f}   (RE: {:.2f}, KL: {:.2f}, NDCG: {:.5f})\\n'\n",
    "            '--> Early stopping: {}/{} (BEST: {:.5f})\\n'.format(\n",
    "        epoch, args.epochs, time_elapsed,\n",
    "        train_loss_epoch, train_re_epoch, train_kl_epoch,\n",
    "        val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch,\n",
    "        e, args.early_stopping_epochs, best_ndcg\n",
    "    ))\n",
    "\n",
    "    # early-stopping\n",
    "    last_epoch = epoch\n",
    "    if val_ndcg_epoch > best_ndcg:\n",
    "        e = 0\n",
    "        best_ndcg = val_ndcg_epoch\n",
    "        # best_model = model\n",
    "        print('->model saved<-')\n",
    "        torch.save(model, dir + args.model_name + '.model')\n",
    "    else:\n",
    "        e += 1\n",
    "        if epoch < args.warmup:\n",
    "            e = 0\n",
    "        if e > args.early_stopping_epochs:\n",
    "            break\n",
    "\n",
    "    # NaN\n",
    "    if math.isnan(val_loss_epoch):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import bottleneck as bn\n",
    "def evaluate_vae(args, model, train_loader, data_loader, epoch, dir, mode):\n",
    "    # set loss to 0\n",
    "    evaluate_loss = 0\n",
    "    evaluate_re = 0\n",
    "    evaluate_kl = 0\n",
    "\n",
    "    ndcg_dist = torch.tensor([], dtype=torch.float)\n",
    "    if mode == 'test':\n",
    "        ndcg_20 = torch.tensor([], dtype=torch.float)\n",
    "        ndcg_10 = torch.tensor([], dtype=torch.float)\n",
    "        recall_50 = torch.tensor([], dtype=torch.float)\n",
    "        recall_20 = torch.tensor([], dtype=torch.float)\n",
    "        recall_10 = torch.tensor([], dtype=torch.float)\n",
    "        recall_5 = torch.tensor([], dtype=torch.float)\n",
    "        recall_1 = torch.tensor([], dtype=torch.float)\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Functions for Evaluation\n",
    "\n",
    "    def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "        '''\n",
    "        normalized discounted cumulative gain@k for binary relevance\n",
    "        ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "        '''\n",
    "        batch_users = X_pred.shape[0]\n",
    "        idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "        topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                           idx_topk_part[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "        # topk predicted score\n",
    "        idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "        # build the discount template\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        tp = torch.tensor(tp, dtype=torch.float)  # ! in order to do operations with torch tensor\n",
    "\n",
    "        DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                             idx_topk].cpu() * tp).sum(dim=1)\n",
    "        IDCG = torch.tensor([(tp[:min(n, k)]).sum()\n",
    "                             for n in (heldout_batch != 0).sum(dim=1)])\n",
    "        #print(DCG, IDCG)\n",
    "        return DCG / IDCG\n",
    "\n",
    "    def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "        batch_users = X_pred.shape[0]\n",
    "\n",
    "        idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "        X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "        X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "        X_true_binary = torch.tensor((heldout_batch > 0), dtype=torch.float)\n",
    "        tmp = torch.tensor(np.logical_and(X_true_binary, X_pred_binary), dtype=torch.float).sum(dim=1)\n",
    "        recall = tmp / np.minimum(k, X_true_binary.sum(dim=1))\n",
    "        return recall\n",
    "\n",
    "    # evaluate\n",
    "    for batch_idx, (train, test) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            train, test = train.cuda(), test.cuda()\n",
    "        train, test = Variable(train), Variable(test) #! volatile deprecated\n",
    "\n",
    "        x = train\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # calculate loss function\n",
    "            loss, RE, KL = model.calculate_loss(x, average=True)\n",
    "\n",
    "            evaluate_loss += loss.data.item()\n",
    "            evaluate_re += -RE.data.item()\n",
    "            evaluate_kl += KL.data.item()\n",
    "\n",
    "            # Calculate NDCG & Recall\n",
    "            pred_val = model.reconstruct_x(x)\n",
    "            # should be removed if not necessary\n",
    "            pred_val = np.array(pred_val)\n",
    "            x = np.array(x)\n",
    "            pred_val[x.nonzero()] = -np.inf\n",
    "\n",
    "            ndcg_dist = torch.cat([ndcg_dist, NDCG_binary_at_k_batch(pred_val, test, k=100)])\n",
    "\n",
    "            if mode == 'test':\n",
    "                ndcg_20 = torch.cat([ndcg_20, NDCG_binary_at_k_batch(pred_val, test, k=20)])\n",
    "                ndcg_10 = torch.cat([ndcg_10, NDCG_binary_at_k_batch(pred_val, test, k=10)])\n",
    "                recall_50 = torch.cat([recall_50, Recall_at_k_batch(pred_val, test, k=50)])\n",
    "                recall_20 = torch.cat([recall_20, Recall_at_k_batch(pred_val, test, k=20)])\n",
    "                recall_10 = torch.cat([recall_10, Recall_at_k_batch(pred_val, test, k=10)])\n",
    "                recall_5 = torch.cat([recall_5, Recall_at_k_batch(pred_val, test, k=5)])\n",
    "                recall_1 = torch.cat([recall_1, Recall_at_k_batch(pred_val, test, k=1)])\n",
    "\n",
    "\n",
    "\n",
    "    # calculate final loss\n",
    "    evaluate_loss /= len(data_loader)  # loss function already averages over batch size\n",
    "    evaluate_re /= len(data_loader)  # re already averages over batch size\n",
    "    evaluate_kl /= len(data_loader)  # kl already averages over batch size\n",
    "    #print(ndcg_dist)\n",
    "    evaluate_ndcg = ndcg_dist.nanmean().data.item()\n",
    "\n",
    "    if mode == 'test':\n",
    "        eval_ndcg100 = \"{:.5f}({:.4f})\".format(evaluate_ndcg, ndcg_dist.std().data.item()/np.sqrt(len(ndcg_dist)))\n",
    "        eval_ndcg20 = \"{:.5f}({:.4f})\".format(ndcg_20.mean().data.item(),ndcg_20.std().data.item()/np.sqrt(len(ndcg_20)))\n",
    "        eval_ndcg10 = \"{:.5f}({:.4f})\".format(ndcg_10.mean().data.item(),ndcg_10.std().data.item()/np.sqrt(len(ndcg_10)))\n",
    "        eval_recall50 = \"{:.5f}({:.4f})\".format(recall_50.mean().data.item(),recall_50.std().data.item()/np.sqrt(len(recall_50)))\n",
    "        eval_recall20 = \"{:.5f}({:.4f})\".format(recall_20.mean().data.item(),recall_20.std().data.item()/np.sqrt(len(recall_20)))\n",
    "        eval_recall10 = \"{:.5f}({:.4f})\".format(recall_10.mean().data.item(),recall_10.std().data.item()/np.sqrt(len(recall_10)))\n",
    "        eval_recall5 = \"{:.5f}({:.4f})\".format(recall_5.mean().data.item(),recall_5.std().data.item()/np.sqrt(len(recall_5)))\n",
    "        eval_recall1 = \"{:.5f}({:.4f})\".format(recall_1.mean().data.item(),recall_1.std().data.item()/np.sqrt(len(recall_1)))\n",
    "\n",
    "\n",
    "\n",
    "    if mode == 'test':\n",
    "        return evaluate_loss, evaluate_re, evaluate_kl, eval_ndcg100, \\\n",
    "               eval_ndcg20, eval_ndcg10, eval_recall50, eval_recall20, eval_recall10, eval_recall5, eval_recall1\n",
    "    else:\n",
    "        return evaluate_loss, evaluate_re, evaluate_kl, evaluate_ndcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_epoch, val_re_epoch, val_kl_epoch, val_ndcg_epoch = evaluate_vae(args, model, train_loader, val_loader, epoch, dir, mode='validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02525036782026291"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ndcg_epoch\n",
    "#evaluate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
